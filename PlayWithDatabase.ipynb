{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching \n",
    "\n",
    "## Types of Data\n",
    "\n",
    "Data contained in databases, documents, e-mails, and other data files for predictive analysis can be categorized either as structured or unstructured data.\n",
    "\n",
    "1. Structured Data\n",
    "\n",
    "Structured data is well organized, follows a consistent order, is relatively easy to search and query, and can be readily accessed and understood by a person or a computer program.A classic example of structured data is an Excel spreadsheet with labeled columns. Such structured data is consistent; column headers-usually brief, accurate descriptions of the content in each column tell you exactly what kind of content to expect.\n",
    "Structured data is usually stored in well-defined schemas such as databases. It's usually tabular, with columns and rows that clearly define its attributes.\n",
    "\n",
    "2. Unstructured Data\n",
    "\n",
    "Unstructured data, on the other hand, tends to be free-form, non-tabular, dispersed, and not easily retrievable; such data requires deliberate intervention to make sense of it. Miscellaneous e-mails, documents, web pages, and files (whether text, audio, and/or video) in scattered locations are examples of unstructured data.\n",
    "\n",
    "It's hard to categorize the content of unstructured data. It tends to be mostly text, it's usually created in a free-form styles, and finding any attributes you can use to describe or group it is no small task.\n",
    "\n",
    "The content of unstructured data is hard to work with or make sense of programmatically. Computer programs cannot analyze or generate reports on such data, simply because it lacks structure, has no underlying dominant characteristic, and individual items of data have no common ground.\n",
    "\n",
    "Unstructured data requires more work to make it useful, so it gets more attention â€” thus tends to consume more time.\n",
    "\n",
    "The resultant newly organized data from those necessary preprocessing steps can then be used in a predictive analytics model. The wholesale transformation of unstructured data however, may have to wait until you have your predictive analytics model up and running.\n",
    "\n",
    "Data mining and text analytics are two approaches to structuring text documents, linking their contents, grouping and summarizing their data, and uncovering patterns in that data. Both disciplines provide a rich framework of algorithms and techniques to mine the text scattered across a sea of documents.\n",
    "\n",
    "### Structured Data Fetching\n",
    "\n",
    "   Structured data (arranged in well defined rows and columns) is spread across different relational databases in different tables.To prepare our dataset for analysis, we have to fetch it from different tables of respective databases.\n",
    "    SQL (Structured  enables us to fetch and manipulate data) using following categories:\n",
    "    \n",
    "1. DDL (Data Definition Language):\n",
    "Defines database-create,drop,alter,truncate,comment,rename etc\n",
    "2. DML (Data Manipulation Language):\n",
    "Manipulates data in database-select,insert,update,delete.\n",
    "3. DCL (Data Control Language):\n",
    "Rights and permission to access database-Grant,revoke\n",
    "4. TCL (Transaction Control Language):\n",
    "Transactions within database-commit,rollback,savepoint,set transactions\n",
    "5. Clauses:\n",
    "First hand Sorting and Filtering data-having,group by,order by,where,top\n",
    "\n",
    "6.Primary and Foreign key - We use Primary Key to uniquely identify each Record in a table. The Primary Key is consist of single or multiple columns. Whereas Foreign Key is a set of multiple columns in a table that indicates the primary key in the alternate table.\n",
    "\n",
    "7.Null Value - We use the term Null Value in SQL to express the missing value. It is a mark in SQL to represent that the value of data does not exist in the database. There is a difference between a Null Value and a zero value or a table that has spaces.\n",
    "\n",
    "8.Subquery - Subquery also called as Nested Query or Insert Query. This means a query in another query, generally insert in WHERE Clause. There are 4 types of Subquery that are with a SELECT statement, INSERT statement, the UPDATE statement, DELETE statement. A Subquery can return a set of records to its primary query.\n",
    "\n",
    "9.Indexes - With the help of SQL indexes we can find out the data easily and quickly, no need to search every row in a database table. That is SQL index quickly load the data. We use this to speed up searches. Index classified in 2 types - Clustered and Non-Clustered Indexes.\n",
    "\n",
    "10.Joins - SQL Joins combines rows from two or more tables. SQL Joins are of 4 types - INNER, LEFT, RIGHT, FULL join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is list of tuples \n",
      "\n",
      "\n",
      "  data_tuple= \n",
      "\n",
      "  [('Graduate', '0', Decimal('1423660')), ('Graduate', '1', Decimal('531549')), ('Graduate', '2', Decimal('412766')), ('Graduate', '3+', Decimal('379986')), ('Not Graduate', None, Decimal('14207')), ('Not Graduate', '0', Decimal('272851')), ('Not Graduate', '1', Decimal('76603')), ('Not Graduate', '2', Decimal('84839')), ('Not Graduate', '3+', Decimal('57656'))] \n",
      "\n",
      "\n",
      "The list object 'data' is converted to pandas DataFrame object 'data_table' \n",
      "\n",
      "\n",
      " table_data= \n",
      "\n",
      "               0     1        2\n",
      "0      Graduate     0  1423660\n",
      "1      Graduate     1   531549\n",
      "2      Graduate     2   412766\n",
      "3      Graduate    3+   379986\n",
      "4  Not Graduate  None    14207\n",
      "5  Not Graduate     0   272851\n",
      "6  Not Graduate     1    76603\n",
      "7  Not Graduate     2    84839\n",
      "8  Not Graduate    3+    57656 \n",
      "\n",
      "\n",
      "Affected Rows due to the Data Manipulation\n",
      "\n",
      " 10 \n",
      "\n",
      "\n",
      "Description of columns \n",
      "\n",
      "  (('Education', 253, None, 252, 252, 0, True), ('Dependents', 253, None, 252, 252, 0, True), ('sum(ApplicantIncome)', 246, None, 42, 42, 0, True))\n"
     ]
    }
   ],
   "source": [
    "# ! /usr/bin/python3.5.2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn is a library for making statistical graphics in Python.\n",
    "# It is built on top of matplotlib and closely integrated with pandas data structures.\n",
    "import seaborn as sns\n",
    "\n",
    "import pymysql\n",
    "\n",
    "#from sqlalchemy import create_engine\n",
    "# python interface with os--allow to use OS functionality in python code\n",
    "# import os\n",
    "# os.system('clear')\n",
    "\n",
    "# read text file from our project folder\n",
    "train=pd.read_csv('train_u6lujuX_CVtuZ9i.txt')\n",
    "test=pd.read_csv('test_Y3wMUE5_7gLdaTN.txt')\n",
    "\n",
    "# Python Database API Specification\n",
    "#API is the acronym for Application Programming Interface, which is a software intermediary that allows\n",
    "#two applications to talk to each other. \n",
    "\n",
    "# here we are using pymysql-PyMySQL is an interface for connecting to a MySQL database server from Python. \n",
    "#It implements the Python Database API v2.0 and contains a pure-Python MySQL client library.\n",
    "\n",
    "# open database connection -connect(parameters... ) is Constructor for creating a connection to the database.\n",
    "# Returns a Connection Object. It takes a number of parameters which are database dependent.\n",
    "# methods associated with connect() or connect() object here 'conn' should respond to .close(),.commit(),.rollback()\n",
    "# and .cursor() method\n",
    "\n",
    "# arguments in .connect method\n",
    "# classpymysql.connections.Connection(host=None, user=None, password='', database=None, port=0, unix_socket=None,\n",
    "#                                     charset='', sql_mode=None, read_default_file=None, conv=None,\n",
    "#                                      use_unicode=None, client_flag=0, cursorclass=<class 'pymysql.cursors.Cursor'>,\n",
    "#                                     init_command=None, connect_timeout=10, ssl=None, read_default_group=None, \n",
    "#                                      compress=None, named_pipe=None, autocommit=False, db=None, passwd=None, \n",
    "#                                      local_infile=False, max_allowed_packet=16777216, defer_connect=False, \n",
    "#                                      auth_plugin_map=None, read_timeout=None, write_timeout=None, bind_address=None,\n",
    "#                                     binary_prefix=False, program_name=None, server_public_key=None)\n",
    "\n",
    "conn = pymysql.connect (db='myproject1',user='root',passwd='mysql',host='localhost',cursorclass=pymysql.cursors.Cursor)\n",
    "\n",
    "\n",
    "# task-write back 'train' dataframe into mysql server as table-'LoanData' under selected database 'selected_db_name'\n",
    "# here I performed this write back task since MySQL server donot have any database and tables\n",
    "# we can also use sqlalchemy package\n",
    "#engine = create_engine('mysql+pymysql://username:password@localhost/selected_db_name')\n",
    "#conn=engine.connect()\n",
    "# pandas function\n",
    "#train.to_sql(\"LoanData_duplicate\",conn,schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None,flavor='mysql')\n",
    "\n",
    "# Exception Handling (try---except block)\n",
    "# \"try\" block==The try block lets you test a block of code for errors.If no exception occurs, the except clause is skipped \n",
    "# and execution of the try statement is finished.\n",
    "# If an exception occurs during execution of the try clause, the rest of the clause is skipped. Then if its type \n",
    "# matches the exception named after the except keyword,the except clause is executed, and then execution continues\n",
    "# after the try statement.\n",
    "# If an exception occurs which does not match the exception named in the except clause, it is passed on to outer try statements; \n",
    "# if no handler is found, it is an unhandled exception and execution stops.\n",
    "# You can use the \"else\" keyword to define a block of code to be executed if no errors raised in \"try\" block\n",
    "# \"finally\" block  lets you execute code, regardless of the result of the try- and except blocks.\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "#with statement\n",
    "\n",
    "#with statement in Python is used in exception handling to make the code cleaner and much more readable.\n",
    "#It simplifies the management of common resources like file streams.\n",
    "#An exception during code execution prevent the connection from closing properly which may introduce several bugs \n",
    "#in the code, i.e. many changes in files do not go into effect until the connection is properly closed.\n",
    "    with conn:\n",
    "\n",
    "#create a cursor object using cursor method--we can use cursor type having return type tuple or dictionary\n",
    "#database cursor, which is used to manage the context of a fetch operation. Cursors created from the same \n",
    "#connection are not isolated, i.e., any changes done to the database by a cursor are immediately visible by \n",
    "#the other cursors. Cursors created from different connections can or can not be isolated, depending on how \n",
    "#the transaction support is implemented (see also the connection's .rollback() and .commit() methods\n",
    "# Methods associated with cursor object are--i) cursor.callproc(arg..) ii) cursor.close() iii) cursor.execute()\n",
    "# iv) cursor.executemany(args) v) cursor.fetchone() vi) cursor.fetchmany() vii) cursor.fetchall() \n",
    "# viii) cursor.nextset() ix) cursor.arraysize() x) cursor.setinputsizes(args..) xi) cursor.setoutputsizes(args..)\n",
    "    \n",
    "        cursor=conn.cursor()   #by default cursor type is tuple that is our result object is 'tuple'--here column names\n",
    "                           # is in numerical form that is column names in table are replaced by 0,1,2...\n",
    "                           # To get the result alongwith original column name,use cursorclass argument in connect method\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#In order to make new table which does not exist in database we can use create table query\n",
    "\n",
    "#Before that we have to check whther table name exist or not in repected database which is a good practice\n",
    "    \n",
    "# Data definition queries-create,drop,comment,rename,alter truncate\n",
    "        \n",
    "        sql_query_ddl1=\"Drop table if exists new_table\"\n",
    "        \n",
    "        sql_query_ddl2=\"create table StudentData(roll_number integer primary key,name varchar(15),marks int,outof int)\"\n",
    "      \n",
    "        # alter table query add/delete columns,add/delete constraints on columns\n",
    "        \n",
    "        sql_query_ddl3=\"Alter table StudentData add column pass_fail varchar(15)\"\n",
    "        \n",
    "        sql_query_ddl4=\"rename table StudentData to StudentReport\"\n",
    "        \n",
    "        # %d-decimal placeholder,%s-string placeholder\n",
    "        \n",
    "        sql_query_dml2=\"insert into StudentReport (roll_number,name,marks,outof,pass_fail) values (%s,%s,%s,%s,%s)\"\n",
    "        \n",
    "        sql_query_dml3=\"update StudentReport set roll_number=2,name='Bharat',marks=55,outof=80,pass_fail='fail'\"\n",
    "        \n",
    "        #truncate table keeps structure of table by deleting all the rows in table.\n",
    "        \n",
    "        sql_query_ddl5=\"truncate table StudentReport\"\n",
    "        \n",
    "#Here LoanData_duplicate table already exist,so we are not creating table here\n",
    "        \n",
    "        # order of sql keywords in query matters \n",
    "        \n",
    "        # order is--select(aggregate function,window,sort,partition),from(selecting from location),\n",
    "        # where(filtering rows),Group by (grouping),having(selection)\n",
    "        \n",
    "        \n",
    "#Data Manipulation queries-insert,select,update,delete\n",
    "\n",
    "        # select-limit <number> --limit on fetching number of rows--limits on selection of rows\n",
    "        # select can take-* (all rows),distinct <col_name>(unique rows,used to avoid data duplication),two or more column\n",
    "        # names,AS-aliases (temporary renaming of column to improve readability),concat('col_name','col_name')\n",
    "        # String Functions-trim(Leading/Trialing/Both '<character/string to be removed> from 'inputstring')\n",
    "        \n",
    "        sql_query1=\"select ApplicantIncome,Education from LoanData_duplicate limit 10\"\n",
    "        sql_query2=\"select ApplicantIncome as Income,Education as Edu from LoanData_duplicate\"\n",
    "        sql_query3=\"select distinct ApplicantIncome from LoanData_duplicate\" \n",
    "        sql_query9=\"select Loan_ID,concat(ApplicantIncome,'+',CoapplicantIncome) as CombinedIncome from LoanData_duplicate\"\n",
    "        \n",
    "        # string functions in select-ASCII(<charornum>),BIN(<num>),bit_length(<str>),char(<num>,<num>),char_length(<str>)\n",
    "        # concat('<str>','<str>'),\n",
    "        \n",
    "        sql_query11=\"select ASCII('L')\"\n",
    "       \n",
    "        \n",
    "        # filtering specific rows--use of where clause--can use operators like <,>,=,<=,=>,!= and keywords-like,\n",
    "        # Between,In,Not like,Not Between,Not In\n",
    "        # But You cannot use aggregate functions (like avg(),count(),first(),last(),max(),min(),sum(),upper(),lower())\n",
    "        # mid(),len(),round(),format()) in where clause\n",
    "        \n",
    "        # Like--filter on the basis of pattern-- wildcard '%' it means that any string of characters (of any length)\n",
    "        # If you want to reduce the number of ORs in your SQL WHERE clause, you should use the IN operator instead.\n",
    "        \n",
    "        sql_query4=\"select * from LoanData_duplicate where ApplicantIncome<=3000 and Education='Graduate' order by ApplicantIncome DESC\"\n",
    "\n",
    "        sql_query5=\"select * from LoanData_duplicate where ApplicantIncome not like '%30%'\"\n",
    "        \n",
    "        sql_query8=\"select Education from LoanData_duplicate where Education like '[GN]%'\"\n",
    "        \n",
    "        sql_query6=\"select * from LoanData_duplicate where ApplicantIncome=3000 or ApplicantIncome=3254 \"\n",
    "        # look at the below query,'or' in replaced by 'in' \n",
    "        sql_query7=\"select * from LoanData_duplicate where ApplicantIncome in (3000,3254)\"\n",
    "        \n",
    "        \n",
    "        # select INTO for creating structure of new table from existing table in database\n",
    "        # i.e create new table of same columns from existing table with data\n",
    "        sql_query10=\"create table new_table select * from LoanData_duplicate where 1=0\"\n",
    "        # add data to created table(from existing one)\n",
    "        sql_query12=\"insert into new_table select * from LoanData_duplicate\"\n",
    "        \n",
    "        \n",
    "        # Group by clause--used in conjunction with aggregate functions to group result set by one or more columns\n",
    "        \n",
    "        sql_query13=\"select Education,Dependents,sum(ApplicantIncome) from LoanData_duplicate group by Education,Dependents\"\n",
    "        \n",
    "        # Having clause--Used in conjunction with aggregate function because where could not work with aggr. funs.\n",
    "        \n",
    "        sql_query14=\"select Education,Dependents,sum(ApplicantIncome) from LoanData_duplicate group by Education,Dependents having sum(ApplicantIncome) >3000\"\n",
    "        \n",
    "        #execute SQL query using execute() method--You can use Transaction also in execute()\n",
    "        \n",
    "        #cursor.execute(sql_query_dml2,(1,Aakash,98,100,Pass))\n",
    "        \n",
    "        cursor.execute(sql_query14)\n",
    "\n",
    "        # Fetch a single row using fetchone() method.\n",
    "       \n",
    "        first_row=cursor.fetchone()\n",
    "\n",
    "        #fetchmany([size=cursor.arraysize])--Fetch first 5 rows defined by size argument--index starts from 1\n",
    "        \n",
    "        #many_rows=cursor.fetchmany(size=5)\n",
    "\n",
    "        # Fetch all rows using fetchall() method.\n",
    "        \n",
    "        all_rows=cursor.fetchall()\n",
    "\n",
    "        #return type of fetchone() and fetchall() methods is 'tuple' object,so type conversion is required\n",
    "        \n",
    "        data_tuple=list(all_rows)\n",
    "\n",
    "        # look at the data that printed in output window\n",
    "        \n",
    "        print(\"The output is list of tuples \\n\\n\\n  data_tuple= \\n\\n \",data, \"\\n\\n\")\n",
    "\n",
    "        # convert this 'data' which is list object into the pandas DataFrame object\n",
    "        \n",
    "        data_table=pd.DataFrame(data)\n",
    "        print(\"The list object 'data' is converted to pandas DataFrame object 'data_table' \\n\\n\\n table_data= \\n\\n\",data_table,\"\\n\\n\")\n",
    "\n",
    "\n",
    "        #.rowcount read-only attribute specifies the number of rows that the last .execute*() produced \n",
    "\n",
    "        #(for DQL statements like SELECT) or affected (for DML statements like UPDATE or INSERT)\n",
    "\n",
    "        #The attribute is -1 in case no .execute*() has been performed on the cursor or the rowcount of the \n",
    "\n",
    "        #last operation is cannot be determined by the interface.\n",
    "        \n",
    "        print(\"Affected Rows due to the Data Manipulation\\n\\n\",cursor.rowcount,\"\\n\\n\")\n",
    "\n",
    "\n",
    "        #cursor.description() is read-only attribute,a sequence of 7-item sequences.\n",
    "\n",
    "        #Each of these sequences contains information describing one result column:\n",
    "\n",
    "        #name\n",
    "\n",
    "        #type_code\n",
    "\n",
    "        #display_size\n",
    "\n",
    "        #internal_size\n",
    "\n",
    "        #precision\n",
    "\n",
    "        #scale\n",
    "\n",
    "        #null_ok\n",
    "        \n",
    "        print(\"Description of columns \\n\\n \",cursor.description)\n",
    "        \n",
    "\n",
    "# Most important topic in data gathering/collection from different tables (databases) is-Constraints on columns \n",
    "# (1. Not Null-indicates that a column cannot store Null value)\n",
    "# (2. unique- ensures that each row for a column must have a unique value)\n",
    "# (3. primary key-combination of Not Null and Unique,ensures that a column or combinations of two or more columns\n",
    "#     have an unique identity which helps to find a particular record in table more easily and quickly)\n",
    "# (4. Foreign key-ensures that the referential integrity of the data in one table to match values in another table)\n",
    "# (5. check-ensures that vallue in column meets specific condition)\n",
    "# (6. default-specifies a default values when specified none for this column)\n",
    "\n",
    "# Above constraints plays an Important role in designing the schema for RDBMS.RDBMS schema is developed to avoid \n",
    "# duplication of data in tables.This is done for two reasons: i) Reducing amount of storage needed to store data\n",
    "# ii) Avoiding unnecessary data conflicts that may creep in because of multiple copies of the same data getting stored.\n",
    "# There are different methods are used while designing the schema for RDBMS.Such methods falls under the Database\n",
    "# Normalization technique.\n",
    "# Normalization is the design process where tables in the database are organized in such a way to avoid redundancy\n",
    "# and dependency of the data. Using normalization of different forms, we can divide data into smaller structures \n",
    "# and establish links between them so that the data is optimally stored.\n",
    "# There are various Normal forms available like 1NF,2NF,3NF and BCNF (Boyce-Codd Normalization form)\n",
    "# When we use these forms,we get an idea of constraints and its use.\n",
    "\n",
    "\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
